{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG Components...\n",
      "RAG Components Initialized.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://10.203.165.181:5001\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received upload request for Python backend.\n",
      "File saved to temporary path: C:\\Users\\adars\\AppData\\Local\\Temp\\tmp28wkmjv5.pdf\n",
      "Loaded 49 base content objects from document.\n",
      "Split document into 491 chunks.\n",
      "Embedded location metadata into document chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2025 19:30:16] \"POST /upload-document HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vectorstore created.\n",
      "RAG QA Chain initialized.\n",
      "Temporary file removed: C:\\Users\\adars\\AppData\\Local\\Temp\\tmp28wkmjv5.pdf\n",
      "Received query request for Python backend.\n",
      "Processing query: '46-year-old male, playing ludo in Pune, 3-month-old insurance policy'\n",
      "Successfully extracted JSON block from raw LLM output.\n",
      "LLM output parsed successfully.\n",
      "Executing sophisticated audit and correction step...\n",
      "Invoking sophisticated auditor model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2025 19:31:37] \"POST /ask-query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auditor model classified the rejection as: 'error'\n",
      "CORRECTION: The auditor found the rejection to be an error. Overriding decision to 'approved'.\n"
     ]
    }
   ],
   "source": [
    "# python-backend/app.py\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Langchain and related imports\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# For DOCX handling\n",
    "try:\n",
    "    from docx import Document as DocxDocument\n",
    "except ImportError:\n",
    "    print(\"python-docx not found. Please install it using: pip install python-docx\")\n",
    "    DocxDocument = None\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY must be set in your .env file\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global variables\n",
    "vectorstore = None\n",
    "llm = None\n",
    "gemini_embedder = None\n",
    "qa_chain = None\n",
    "\n",
    "def initialize_rag_components():\n",
    "    global llm, gemini_embedder\n",
    "    print(\"Initializing RAG Components...\")\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    gemini_embedder = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\"\n",
    "    )\n",
    "    print(\"RAG Components Initialized.\")\n",
    "\n",
    "initialize_rag_components()\n",
    "\n",
    "PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an exceptionally precise and rigorous insurance policy assistant. Your core function is to act as a highly reliable RAG (Retrieval-Augmented Generation) system, providing definitive and fully justified decisions based solely on the provided \"Policy Clauses\" and \"User Query.\"\n",
    "    Absolute Principle: Your final \"decision\" (approved/rejected) and every part of your \"justification\" MUST be perfectly aligned and directly, verifiably supported by the text within the \"Policy Clauses.\" Do NOT invent information or make assumptions beyond what is explicitly stated. If a claim is rejected, the justification MUST clearly explain why it was rejected, referencing the specific clauses that lead to the rejection.\n",
    "    Your Step-by-Step Reasoning Process (Internal Thought Process - Do NOT output this):\n",
    "    Deconstruct the User Query:\n",
    "    Identify Core Intent: What is the user fundamentally asking for (e.g., claim approval, payout amount for a specific event)?\n",
    "    Extract Key Parameters: Parse and structure all relevant details from the \"User Query.\" Prioritize:\n",
    "    Insured's Age: (e.g., \"65 years old\")\n",
    "    Claim/Procedure Type: (e.g., \"medical check-up,\" \"car accident,\" \"life insurance claim\")\n",
    "    Specific Event/Condition: (e.g., \"broken arm,\" \"stolen vehicle,\" \"diagnosis of cancer\")\n",
    "    Location of Event: (e.g., \"within the USA,\" \"abroad\")\n",
    "    Policy Timelines: (e.g., \"policy active for 3 years,\" \"event occurred last month\")\n",
    "    Financial Details: (e.g., \"deductible paid,\" \"premium status\")\n",
    "    Handle Ambiguity/Missing Info: If any critical parameter is vague, incomplete, or absent, explicitly note this internally. Consider what information is missing that would be required by a typical policy clause for approval.\n",
    "    Precise Clause Retrieval & Semantic Matching:\n",
    "    Deep Reading: Thoroughly read and semantically understand every \"Policy Clause.\" Focus on conditions, exclusions, definitions, and payout schedules. Avoid superficial keyword matching.\n",
    "    Relevance Mapping: Identify all and only the clauses that are directly and unequivocally relevant to the parsed details from the user's query. This requires careful inference based on meaning, not just words.\n",
    "    Cross-Reference: Compare the extracted query parameters against the conditions and rules in the relevant clauses.\n",
    "    Rigorous Decision Logic & Calculation:\n",
    "    Decision Determination: Based strictly on the cross-referenced information, determine if the claim is \"APPROVED\" or \"REJECTED.\"\n",
    "    If all conditions for approval are met and no exclusions apply, the decision MUST be \"approved.\"\n",
    "    Amount Calculation: If the decision is \"approved,\" calculate the exact \"amount\" based only on the payout rules and schedules specified in the relevant clauses. If the amount is not explicitly stated or cannot be calculated from the provided clauses, set it to 0 and explain why in the justification. If the decision is \"rejected\", the amount should be 0.\n",
    "    Comprehensive Justification & Clause Mapping (MUST Align with Final Decision):\n",
    "    Construct Explanation: Formulate a clear and concise explanation that directly supports the determined \"decision\" (approved or rejected) and \"amount.\"\n",
    "    Verbatim Clause Quotation: For every point in your explanation, identify the exact, verbatim text of the policy clause(s) that directly support it.\n",
    "    Precise Location: Provide the precise \"location\" for each quoted clause (e.g., \"Page X, Clause Y\", \"Section Z, Paragraph A\"). If the location is not provided in the input, state \"Location not specified in context.\"\n",
    "    Address Missing Information / Rejection Reasons (Crucially):\n",
    "    If the decision is \"rejected,\" your justification MUST explicitly state the reason for rejection. This includes:\n",
    "    Clearly stating what specific information was missing from the \"User Query\" that prevented approval.\n",
    "    Explaining why that information was critical by referencing the specific clause(s) that require it.\n",
    "    Referencing any applicable exclusion clauses that led to the rejection.\n",
    "    Do NOT make up information. You CAN interpret the meaning and implications from the document, but all facts must originate from the provided clauses.\n",
    "    Output Format (Strictly ONLY this JSON object - No other text, no conversational remarks):\n",
    "\n",
    "    {{\n",
    "      \"decision\": \"approved/rejected\",\n",
    "      \"amount\": 100000,\n",
    "      \"justification\": {{\n",
    "        \"clauses\": [\n",
    "          {{\n",
    "            \"text\": \"...\",\n",
    "            \"location\": \"Page X, Clause Y\"\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    User Query:\n",
    "    {question}\n",
    "\n",
    "    Policy Clauses:\n",
    "    {context}\n",
    "\n",
    "    Final Review (Internal - Do NOT output): Before generating the JSON, mentally cross-check:\n",
    "    Is the decision field consistent with the justification provided?\n",
    "    Is the amount correctly calculated or justified as 0?\n",
    "    Are all quoted clauses verbatim and correctly attributed with location?\n",
    "    If rejected, is the reason for rejection clearly explained in the justification, with clause references?\n",
    "    Is the output only the JSON object?\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# In app.py, REPLACE the old function with this final version.\n",
    "\n",
    "def verify_and_correct_decision(initial_result: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Audits a 'rejected' decision using a more sophisticated logic to prevent\n",
    "    false approvals for irrelevant queries.\n",
    "\n",
    "    Args:\n",
    "        initial_result: The dictionary parsed from the initial RAG JSON output.\n",
    "\n",
    "    Returns:\n",
    "        The corrected dictionary.\n",
    "    \"\"\"\n",
    "    print(\"Executing sophisticated audit and correction step...\")\n",
    "    # We only intervene if the decision was 'rejected'.\n",
    "    if initial_result.get(\"decision\") != \"rejected\":\n",
    "        return initial_result\n",
    "\n",
    "    try:\n",
    "        clauses = initial_result.get(\"justification\", {}).get(\"clauses\", [])\n",
    "        # If there's no justification at all, trust the rejection (likely for an irrelevant query)\n",
    "        if not clauses:\n",
    "            print(\"No justification clauses found. Assuming rejection is correct for irrelevant query.\")\n",
    "            return initial_result\n",
    "        \n",
    "        justification_text = \"\\n\".join([clause.get(\"text\", \"\") for clause in clauses])\n",
    "\n",
    "        # Create a new, separate LLM instance for the audit.\n",
    "        auditor_llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0.8\n",
    "        )\n",
    "\n",
    "        # A much more sophisticated prompt to handle both error types.\n",
    "        audit_prompt = f\"\"\"\n",
    "        You are an insurance claim auditor. Your task is to review a 'rejected' claim decision based on its justification.\n",
    "\n",
    "        Here is the justification provided for the rejection, which quotes clauses from a policy document:\n",
    "        --- JUSTIFICATION TEXT ---\n",
    "        {justification_text}\n",
    "        ---\n",
    "\n",
    "        Analyze this justification and determine if the 'rejected' decision was an error. Consider two scenarios:\n",
    "\n",
    "        1.  **Error**: The justification text is weak, irrelevant, or contradicts a 'rejected' decision. For example, it only lists positive-sounding clauses of coverage, or the clauses have nothing to do with a typical insurance claim. This would make the rejection an 'error'.\n",
    "\n",
    "        2.  **Correct**: The justification text clearly explains the rejection (e.g., by citing an exclusion) OR the quoted text shows that the user's request (which you cannot see) is simply not a topic covered by the policy at all. This would make the rejection 'correct'.\n",
    "\n",
    "        Based on the justification text, which scenario is more likely?\n",
    "        Answer with only the single word 'error' or 'correct'.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Invoking sophisticated auditor model...\")\n",
    "        audit_response = auditor_llm.invoke(audit_prompt)\n",
    "        audit_answer = audit_response.content.strip().lower()\n",
    "        print(f\"Auditor model classified the rejection as: '{audit_answer}'\")\n",
    "\n",
    "        # *** FINAL LOGIC ***\n",
    "        # Only override the decision if the auditor explicitly classifies the rejection as an 'error'.\n",
    "        if \"error\" in audit_answer:\n",
    "            print(\"CORRECTION: The auditor found the rejection to be an error. Overriding decision to 'approved'.\")\n",
    "            corrected_result = initial_result.copy()\n",
    "            corrected_result[\"decision\"] = \"approved\"\n",
    "            return corrected_result\n",
    "        else:\n",
    "            print(\"Audit confirms the 'rejected' decision is correct. No changes made.\")\n",
    "            return initial_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the audit step: {e}\")\n",
    "        return initial_result\n",
    "\n",
    "@app.route('/upload-document', methods=['POST'])\n",
    "def upload_document():\n",
    "    global vectorstore, qa_chain\n",
    "    print(\"Received upload request for Python backend.\")\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"message\": \"No file part\"}), 400\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({\"message\": \"No selected file\"}), 400\n",
    "    if file:\n",
    "        file_extension = file.filename.split('.')[-1].lower()\n",
    "        if file_extension not in ['pdf', 'txt', 'docx']:\n",
    "            return jsonify({\"message\": \"Unsupported file type. Please upload PDF, TXT, or DOCX.\"}), 400\n",
    "        temp_file_path = None\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_extension}\") as temp_file:\n",
    "                file.save(temp_file.name)\n",
    "                temp_file_path = temp_file.name\n",
    "            print(f\"File saved to temporary path: {temp_file_path}\")\n",
    "            pages = []\n",
    "            if file_extension == 'pdf':\n",
    "                loader = PyPDFLoader(temp_file_path)\n",
    "                pages = loader.load()\n",
    "            elif file_extension == 'txt':\n",
    "                loader = TextLoader(temp_file_path)\n",
    "                pages = loader.load()\n",
    "            elif file_extension == 'docx':\n",
    "                if DocxDocument:\n",
    "                    doc = DocxDocument(temp_file_path)\n",
    "                    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "                    pages = [Document(page_content=full_text, metadata={\"source\": file.filename, \"file_type\": \"docx\"})]\n",
    "                else:\n",
    "                    raise ImportError(\"python-docx is not installed. Cannot process .docx files.\")\n",
    "            print(f\"Loaded {len(pages)} base content objects from document.\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=100,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "            )\n",
    "            docs = text_splitter.split_documents(pages)\n",
    "            print(f\"Split document into {len(docs)} chunks.\")\n",
    "\n",
    "            # === MODIFICATION: EMBED METADATA INTO DOCUMENT CONTENT ===\n",
    "            for doc in docs:\n",
    "                # Format the metadata into a readable string.\n",
    "                # PyPDFLoader is 0-indexed, so we add 1 for human-readable page numbers.\n",
    "                source = doc.metadata.get('source', 'N/A').split('/')[-1] # Get just the filename\n",
    "                page = doc.metadata.get('page')\n",
    "                location_str = f\"Source: {source}\"\n",
    "                if page is not None:\n",
    "                    location_str += f\", Page: {page + 1}\"\n",
    "\n",
    "                # Prepend the location to the page content.\n",
    "                doc.page_content = f\"--- METADATA ---\\nLocation: {location_str}\\n--- CONTENT ---\\n{doc.page_content}\"\n",
    "            print(\"Embedded location metadata into document chunks.\")\n",
    "            # === END MODIFICATION ===\n",
    "\n",
    "            vectorstore = FAISS.from_documents(docs, embedding=gemini_embedder)\n",
    "            print(\"FAISS vectorstore created.\")\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "            qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\"prompt\": PROMPT_TEMPLATE}\n",
    "            )\n",
    "            print(\"RAG QA Chain initialized.\")\n",
    "            return jsonify({\"message\": f\"File '{file.filename}' processed successfully. RAG pipeline initialized.\"}), 200\n",
    "        except ImportError as ie:\n",
    "            print(f\"Error: {ie}\")\n",
    "            return jsonify({\"message\": f\"Server error: {str(ie)}. Please ensure all required libraries are installed.\"}), 500\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            return jsonify({\"message\": f\"Error processing file: {str(e)}\"}), 500\n",
    "        finally:\n",
    "            if temp_file_path and os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "                print(f\"Temporary file removed: {temp_file_path}\")\n",
    "\n",
    "@app.route('/ask-query', methods=['POST'])\n",
    "def ask_query():\n",
    "    global qa_chain\n",
    "    print(\"Received query request for Python backend.\")\n",
    "    if not qa_chain:\n",
    "        return jsonify({\"message\": \"No document uploaded or RAG pipeline not initialized.\"}), 400\n",
    "    data = request.get_json()\n",
    "    query = data.get('query')\n",
    "    if not query:\n",
    "        return jsonify({\"message\": \"No query provided\"}), 400\n",
    "    try:\n",
    "        print(f\"Processing query: '{query}'\")\n",
    "        result = qa_chain({\"query\": query})\n",
    "        raw_output_string = result.get('result', '')\n",
    "        json_match = re.search(r'\\{.*\\}', raw_output_string, re.DOTALL)\n",
    "        cleaned_json_string = \"\"\n",
    "        if json_match:\n",
    "            cleaned_json_string = json_match.group(0)\n",
    "            print(\"Successfully extracted JSON block from raw LLM output.\")\n",
    "        else:\n",
    "            print(\"WARNING: Could not find a JSON block in the LLM output.\")\n",
    "            cleaned_json_string = raw_output_string\n",
    "        try:\n",
    "            parsed_output = json.loads(cleaned_json_string)\n",
    "            print(\"LLM output parsed successfully.\")\n",
    "            \n",
    "            final_output = verify_and_correct_decision(parsed_output)\n",
    "            \n",
    "            return jsonify(final_output), 200\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"ERROR: Failed to parse even the cleaned string as JSON.\")\n",
    "            error_response = {\n",
    "                \"error\": \"Failed to parse LLM output as JSON\", \n",
    "                \"raw_output\": raw_output_string\n",
    "            }\n",
    "            return jsonify(error_response), 500\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in the RAG chain: {e}\")\n",
    "        return jsonify({\"message\": f\"Error processing query: {str(e)}\"}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5001, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df3fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
